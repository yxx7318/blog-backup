# 参考博客

- [GPT实战系列-ChatGLM3本地部署CUDA11+1080Ti+显卡24G实战方案_chatglm3 github-CSDN博客](https://blog.csdn.net/Alex_StarSky/article/details/134188318)
- [体验ChatGLM3@Windows11 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/664029189?utm_id=0)
- [2023最新pytorch快速安装指南(超详细版)_python_脚本之家 (jb51.net)](https://www.jb51.net/python/302744e4p.htm)
- [Flash-attention 2.3.2 Windows下编译安装 - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv27137329/)

> Flash Attention 是一种用于加速自注意力机制的算法，它可以提高计算效率并减少内存使用，尤其是在处理长序列时。
> 警告信息表明，当前的 PyTorch 版本没有编译 Flash Attention 功能。这可能是因为 Flash Attention 是一个相对较新的特性，可能需要从源代码编译 PyTorch 或使用预编译的带有 Flash Attention 支持的 PyTorch 版本。
> 如果你在使用大型模型（如 ChatGLM3-6b）进行训练或推理时遇到这个警告，你可能会错过一些性能优化。不过，这通常不会影响功能的正常运行，只是可能运行得慢一些，或者使用更多的内存。
>
> 要利用 Flash Attention 来提高性能，你可以按照以下步骤操作：
> 确保 PyTorch 版本支持 Flash Attention：首先，你需要确保你的 PyTorch 版本支持 Flash Attention。Flash Attention 是一个相对较新的特性，可能需要从源代码编译 PyTorch 或使用预编译的带有 Flash Attention 支持的 PyTorch 版本。你可以查看 PyTorch 的官方文档或 GitHub 仓库，以了解如何获取支持 Flash Attention 的版本。
> 使用支持 Flash Attention 的 Transformer 模型：一旦你有了支持 Flash Attention 的 PyTorch 版本，你可以开始使用支持 Flash Attention 的 Transformer 模型。一些 Transformer 模型库（如 Hugging Face Transformers）可能已经集成了 Flash Attention。你可以查看这些库的文档，了解如何使用 Flash Attention。
> 在训练或推理时启用 Flash Attention：在使用支持 Flash Attention 的 Transformer 模型进行训练或推理时，确保 Flash Attention 已启用。这可能需要设置特定的配置选项或使用特定的模型类。具体操作取决于你使用的库或框架。
> 监控性能和内存使用情况：在使用 Flash Attention 时，监控模型的性能和内存使用情况以确保 Flash Attention 正在提供预期的性能改进。你可以使用性能分析工具和内存分析工具来帮助你进行监控。
> 通过利用 Flash Attention，你可以提高 Transformer 模型的性能，特别是在处理长序列时。然而，要成功利用 Flash Attention，你需要确保你的 PyTorch 版本和 Transformer 模型都支持这一特性。