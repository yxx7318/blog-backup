# 参考博客

- [GPT实战系列-ChatGLM3本地部署CUDA11+1080Ti+显卡24G实战方案_chatglm3 github-CSDN博客](https://blog.csdn.net/Alex_StarSky/article/details/134188318)
- [体验ChatGLM3@Windows11 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/664029189?utm_id=0)
- [2023最新pytorch快速安装指南(超详细版)_python_脚本之家 (jb51.net)](https://www.jb51.net/python/302744e4p.htm)
- [Flash-attention 2.3.2 Windows下编译安装 - 哔哩哔哩 (bilibili.com)](https://www.bilibili.com/read/cv27137329/)

## Flash Attention

> Flash Attention是一种用于加速自注意力机制的算法，它可以提高计算效率并减少内存使用，尤其是在处理长序列时

直接运行模型，信息警告信息表明，当前的PyTorch版本没有编译Flash Attention功能。如果在使用大型模型（如 ChatGLM3-6b）进行训练或推理时遇到这个警告，可能会错过一些性能优化。不过，这通常不会影响功能的正常运行，只是可能运行得慢一些，或者使用更多的内存。这可能是因为Flash Attention是一个相对较新的特性，可能需要从源代码编译 PyTorch或使用预编译的带有Flash Attention支持的PyTorch版本，要利用 Flash Attention 来提高性能，可以按照以下步骤操作：

- 确保PyTorch版本支持Flash Attention：首先，需要确保PyTorch版本支持Flash Attention。Flash Attention是一个相对较新的特性，可能需要从源代码编译PyTorch或使用预编译的带有 Flash Attention 支持的PyTorch版本。可以查看PyTorch的官方文档或GitHub仓库，以了解如何获取支持Flash Attention的版本
- 使用支持Flash Attention的Transformer模型：有了支持Flash Attention的PyTorch版本，可以开始使用支持Flash Attention的Transformer模型。一些 Transformer 模型库（如Hugging Face Transformers）可能已经集成了Flash Attention。可以查看这些库的文档，了解如何使用Flash Attention
- 在训练或推理时启用Flash Attention：在使用支持Flash Attention的Transformer模型进行训练或推理时，确保Flash Attention已启用。这可能需要设置特定的配置选项或使用特定的模型类。具体操作取决于使用的库或框架
- 监控性能和内存使用情况：在使用Flash Attention时，监控模型的性能和内存使用情况以确保Flash Attention正在提供预期的性能改进。你可以使用性能分析工具和内存分析工具来帮助你进行监控
- 通过利用Flash Attention，可以提高Transformer模型的性能，特别是在处理长序列时。然而，要成功利用Flash Attention，需要确保PyTorch版本和 Transformer模型都支持这一特性