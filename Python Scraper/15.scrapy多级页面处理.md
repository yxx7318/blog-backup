# scrapy多级页面处理

## 准备工作

> 爬取电影天堂的数据

创建爬虫项目

```
scrapy startproject scrapy_dytt
```

切换目录

```
cd scrapy_dytt\scrapy_dytt\spiders
```

创建爬虫文件

```
scrapy genspider dytt https://www.dytt8.net/index2.htm
```

注释settings.py中的robots协议遵守，在dytt.py中添加后缀`index2.htm`，前面http的`s`可加可不加

运行爬虫为

```
scrapy crawl dytt
```

## items.py

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class ScrapyDyttItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()
    src = scrapy.Field()

```

## piplines.py

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter
import urllib.request


class ScrapyDyttPipeline:
    def __init__(self):
        self.fp = None

    def open_spider(self, spider):
        self.fp = open('movie.json', 'w', encoding='utf-8')

    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item

    def close_spider(self, spider):
        self.fp.close()


# 开启多条管道，在定义管道类后需要在settings.py中添加管道
class ScrapyDyttDownloadPipeline:
    def process_item(self, item, spider):
        url = item.get('src')
        # 文件不允许出现以下任何特殊字符/ \ : * ? " < > |
        filename = 'movie/' + item.get('name').replace("/", "").replace("\\", "").replace(":", "").replace("*", "").replace("?", "").replace("\"", "").replace("<", "").replace(">", "").replace("|", "") + '.jpg'
        urllib.request.urlretrieve(url=url, filename=filename)
        return item

```

## dytt.py

```python
import scrapy
from scrapy_dytt.items import ScrapyDyttItem
import os  # os.mkdir("单级目录") os.makedirs("多级目录")


class DyttSpider(scrapy.Spider):
    name = "dytt"
    allowed_domains = ["www.dytt8.net"]
    start_urls = ["http://www.dytt8.net/index2.htm"]

    folder_name = 'movie'
    if os.path.exists(folder_name):
        print("目录\"" + folder_name + "\"已存在，不可重复创建")
    else:
        os.makedirs(folder_name)

    def parse(self, response):
        a_list = response.xpath('//div/div[3]/div[2]/div[2]/div[1]/div/div[2]/div[2]//a[2]')

        for a in a_list:
            # 获取第一页的name 和 要点击的链接
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()

            # 第二页的地址是
            url = 'https://www.dytt8.net' + href

            # 对第二页的链接发起访问，使用自身的方法，通过meta以字典的方式将参数传递过去
            yield scrapy.Request(url=url, callback=self.parse_second, meta={'name': name})

    def parse_second(self, response):
        # 注意 如果拿不到数据的情况下  一定检查你的xpath语法是否正确
        src = response.xpath('//div[@id="Zoom"]//img/@src').extract_first()
        # 接受到请求的那个meta参数的值
        name = response.meta['name']
        # 存入管道封装
        movie = ScrapyDyttItem(src=src, name=name)
        yield movie

```
