# 爬取站长素材网站图片

> 有时候会被识别为手机端，如果无法打开，修改`url_base="https://m.sc.chinaz.com"`即可
>
> 直接爬取的是压缩图，此时只需要删除图片文件名后面的`"_s"`即可获取高清图

```python
import urllib.request
import os  # os.mkdir("单级目录") os.makedirs("多级目录")
import time
import random
from lxml import etree

headers = {
    "User-Agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 '
                  'Safari/537.36 Edg/111.0.1661.62 ',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    # 'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
    'Cache-Control': 'max-age=0',
    'Connection': 'keep-alive',
    'Cookie': 'cz_statistics_visitor=956c823b-60c7-844c-dad0-ddb4e8c1b388; __bid_n=1876ed9a76f6f65d7e4207; FPTOKEN=ARNjh3BOzJMZyFqFSRz0HVBgwN3QL0lQU5uwXT1yLeKk4gTQZ+C1IUik1gid32QBNFwy58G4bpkvQmq0mdtOljz8ouE7/s9HywNW8CXu9ws2ntFEkMt+pfDzJWaZbav7Oc5KZvNidgxnQOrsw/O2VyS6b//W4xvg44dKuYVXHfrZW8lL6qONfx166a+dhrZUJmUFhWxAuzAyf++tzV0dqGyHzQV/KnEu6txFrmoYricQolna4aJanLAuQJm4nqEe7bmJYOTI8qr2UZNilJmHa8xGcbs3OSe27VfqsQC0TqRXjaG4uC/DjKuz5vhwzgJMZAGJ1hfaUsv2IBaXbv74FmqXhxIKPLYZUety+TV8mhtSVUj89XimVCal4u9dty3STdUrQqAgHNDwLRVuCRKIbQ==|HmQ0Q1+aGI+J7bDXGEbGEn5XGdI+9ih4QjegeseZmfo=|10|06410ee3c80241caeeb34c1a097e56fd; Hm_lvt_398913ed58c9e7dfe9695953fb7b6799=1681191971; Hm_lpvt_398913ed58c9e7dfe9695953fb7b6799=1681220548',
    'Host': 'sc.chinaz.com',
    'Referer': 'https://sc.chinaz.com/tupian/dongwutupian.html',
    'sec-ch-ua': '"Chromium";v="112", "Microsoft Edge";v="112", "Not:A-Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'same-origin',
    'Sec-Fetch-User': '?1',
    'Upgrade-Insecure-Requests': '1',
}


# 创建主文件目录，并下载主页面
def folder_main(folder_main_name):
    # 创建主文件夹
    folder(folder_main_name)
    # 获取主页面信息
    with open('chinaz/chinaz.html', 'w', encoding='utf-8') as fp:
        # 修改后缀可以修改基准
        fp.write(https('/tupian/dongwutupian.html'))


# 接受url_ending，返回获取的页面信息，当没有url_ending时，使用传过来的url
def https(url_ending=None, url_base=None):
    if url_ending is not None:
        url_base = "https://sc.chinaz.com"
        url = url_base + url_ending
    else:
        url = url_base
    request = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(request)
    return response.read().decode("UTF-8")


def folder(folder_name):
    if os.path.exists(folder_name):
        print("目录\"" + folder_name + "\"已存在，不可重复创建")
    else:
        os.makedirs(folder_name)
    time.sleep(0.7)


def picture(picture_url, picture_name, folder_name, small_folder_name, page):
    # 拼接地址
    picture_url = 'https:' + picture_url
    # 图片保存位置为：大分类+小分类+页数+图片名称
    filename = 'chinaz/' + folder_name + '/' + small_folder_name + '/' + str(page + 1) + '/' + picture_name + '.jpg'
    # 下载图片
    urllib.request.urlretrieve(url=picture_url, filename=filename)
    time.sleep(random.random() + 0.5)


# 处理本地html文件信息，返回tree
def parser_information(address):
    parser = etree.HTMLParser(encoding="UTF-8")
    return etree.parse(address, parser=parser)


# 处理发送过来的html信息，返回tree
def HTML_information(url_ending=None, url_base=None):
    return etree.HTML(https(url_ending=url_ending, url_base=url_base))


if __name__ == '__main__':
    # 获取用户输入的信息
    big_start = int(input("输入大分类起始页面"))
    big_end = int(input("输入大分类结束页面"))
    small_start = int(input("输入小分类起始页面"))
    small_end = int(input("输入小分类结束页面"))
    page_start = int(input("输入下载的起始页数"))
    page_end = int(input("输入下载的终止页数"))
    picture_number = int(input("输入每页下载的数量"))

    # 创建主存储页并下载
    folder_main("chinaz")

    # 获取大分类信息
    tree = parser_information("chinaz/chinaz.html")
    # 大分类的小分类链接
    folder_url = tree.xpath('//div[@class="one-class-cont screen-bule-a-div"]//@href')
    # 大分类图片小分类链接
    folder_name = tree.xpath('//div[@class="one-class-cont screen-bule-a-div"]//a/text()')
    print("大分类名称总和", folder_name)

    for i in range(big_start - 1, big_end):
        # 批量创建大分类图片目录
        folder('chinaz/' + folder_name[i])
        content = https(folder_url[i])
        # 批量访问并下载
        with open('chinaz/' + folder_name[i] + '/' + folder_name[i] + '.html', 'w', encoding='utf-8') as fp:
            fp.write(content)  # 批量保存到对应目录下
            # 随机休眠防止被封
        time.sleep(random.random() + 1)

    # 循环大分类
    for i in range(big_start - 1, big_end):
        # 进入大分类
        tree = parser_information("chinaz/" + folder_name[i] + "/" + folder_name[i] + ".html")
        print("输出大分类", folder_name[i])
        # 获取大分类下小分类图片链接和名称
        small_folder_name = tree.xpath('//div[@class="two-class screen-bule-a-div clearfix"]/a/text()')
        print("输出小分类名称", small_folder_name)
        small_folder_url = tree.xpath('//div[@class="two-class screen-bule-a-div clearfix"]/a/@href')
        print("输出小分类地址", small_folder_url)
        for j in range(small_start - 1, small_end):
            # 进入小分类的第一页
            tree = HTML_information(small_folder_url[j])
            # 首页数据进行处理
            page_url = "//link[@media=\"only screen and(max-width: 640px)\"]/@href"
            # 获取到页面跳转的关键地址
            key_url = tree.xpath(page_url)
            print("基于此url进行页面跳转", key_url)

            # 下载小分类下的图片
            for page in range(page_start - 1, page_end):
                # 创建小分类下页数的文件夹
                folder('chinaz/' + folder_name[i] + '/' + small_folder_name[j] + '/' + str(page + 1))

                # 确定目标页面
                # 如果是第一页
                if page == 0:
                    picture_page = key_url[0]
                    print("目标页面地址", picture_page)
                # 不为第一页
                else:
                    picture_page = key_url[0].replace(".html", "") + "_" + str(page + 1) + '.html'
                    print("目标页面地址", picture_page)

                # 访问目标页面，对目标页面进行信息爬取
                tree = HTML_information(url_base=picture_page)
                picture_url = tree.xpath('//div[@class="tupian-list com-img-txt-list"]//img/@data-original')
                picture_name = tree.xpath('//div[@class="tupian-list com-img-txt-list"]//img/@alt')
                # 对目标页面图片进行循环下载
                for k in range(0, picture_number):
                    # 图片下载地址，图片名称，大分类名称，小分类名称，页数
                    picture(picture_url[k], picture_name[k], folder_name[i], small_folder_name[j], page)

    print("已经完成所有下载任务")

```

