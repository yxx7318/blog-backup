# AI科普

## AIGC与生成式AI

<img src="img/AI科普/image-20240122111924479.png" alt="image-20240122111924479" style="zoom:67%;" />

<img src="img/AI科普/image-20240122112121748.png" alt="image-20240122112121748" style="zoom:67%;" />

<img src="img/AI科普/image-20240122112230046.png" alt="image-20240122112230046" style="zoom:67%;" />

### 学习方式

**监督学习**

<img src="img/AI科普/image-20240122113227177.png" alt="image-20240122113227177" style="zoom:67%;" />

> 监督学习之分类
>
> <img src="img/AI科普/image-20240122113344655.png" alt="image-20240122113344655" style="zoom:67%;" />
>
> 监督学习之回归
>
> <img src="img/AI科普/image-20240122113448486.png" alt="image-20240122113448486" style="zoom:67%;" />

**无监督学习**

<img src="img/AI科普/image-20240122113611487.png" alt="image-20240122113611487" style="zoom:67%;" />

**强化学习**

<img src="img/AI科普/image-20240122113702968.png" alt="image-20240122113702968" style="zoom:67%;" />

## 大语言模型

<img src="img/AI科普/image-20240122113841227.png" alt="image-20240122113841227" style="zoom: 67%;" />

> 不是所有的生成式AI都是大语言模型，而所有的大语言是否都是生成式AI也存在些许争议
>
> <img src="img/AI科普/image-20240122114244478.png" alt="image-20240122114244478" style="zoom:67%;" />
>
> 谷歌的BERT在理解上下文的能力较强，被谷歌用于搜索上，也被用于情感分析、文本分类等任务，但BERT不擅长文本生成，特别是连贯的长文本生成，所以有人认为此模型不属于生成式AI的范畴
>
> <img src="img/AI科普/image-20240122114328704.png" alt="image-20240122114328704" style="zoom:67%;" />

<img src="img/AI科普/image-20240122112941868.png" alt="image-20240122112941868" style="zoom:67%;" />

**GPT-3训练数据**

<img src="img/AI科普/image-20240122114940215.png" alt="image-20240122114940215" style="zoom:67%;" />

### Transformer架构

有能力学习输入序列中所有词的相关性和上下文，不会受到短时记忆的影响

<img src="img/AI科普/image-20240122113052426.png" alt="image-20240122113052426" style="zoom:67%;" />

> GPT为G(Generative)生成式、P(Pre-trained)预训练的、T(Transformer)

在Transformer之前，循环神经网络(RNN)

<img src="img/AI科普/image-20240122115417982.png" alt="image-20240122115417982" style="zoom:67%;" />

> 后面的优化版本LSTM也依旧受到限制

## 原始Transformer原理

<img src="img/AI科普/image-20240122142304537.png" alt="image-20240122142304537" style="zoom:67%;" />

**编码器**

通过编码器的嵌入层将每个Token转化为词向量

<img src="img/AI科普/image-20240122142022040.png" alt="image-20240122142022040" style="zoom:67%;" />

再进行位置编码，理解顺序关系

<img src="img/AI科普/image-20240122142516736.png" alt="image-20240122142516736" style="zoom:67%;" />

最后通过编码器将输入转化为一种更抽象的表示形式——向量，里面既保留了输入文本的词汇信息和顺序关系，也捕捉了语法语义上的关键特征。关键特征的获取通过编码器的自注意力机制，这个机制通过计算每对词之间的相关性来决定注意力的权重，这样输出的结果里面不仅包含词本身的信息，还融合了上下文中的相关信息。可以包含多个注意力头，不同的注意力头可以用来关注不同的特征或者方面，且它们之间可以做并行运算

<img src="img/AI科普/image-20240122142704070.png" alt="image-20240122142704070" style="zoom:67%;" />

在多头注意力后面还有一个前馈神经网络，它会对自注意力模块的输出进行进一步的处理，增强模型的表达能力。且编码器本身也可以有多个，帮助模型更为全面的理解数据

**解码器**

和编码器一样，文本也需要经过嵌入层和位置编码，然后被输入进多头自注意力层

<img src="img/AI科普/image-20240122143832675.png" alt="image-20240122143832675" style="zoom:67%;" />

但这个多头自注意层和编码器中的有些不一样，编码器在处理各个词的时候，它会关注输入序列里所有其它词，但解码器中自注意力只会关注这个词和它前面的其它词，后面的词需要被遮住，这么做是为了确保解码器生成文本时，遵循正确的时间顺序。预测下一词时，只是用前面的词作为上下文。这种多头注意力也被叫做带掩码的多头自注意力。注意力会捕捉编码器的输入和解码器即将生成的输出之间的对应关系，从而将原始输入序列的信息融合到输出序列的生成过程中

<img src="img/AI科普/image-20240122144708401.png" alt="image-20240122144708401" style="zoom:67%;" />

解码器里的前馈神经网络作用和编码器里的类似，也是通过额外的计算，来增强模型的表达能力

<img src="img/AI科普/image-20240122145228813.png" alt="image-20240122145228813" style="zoom:67%;" />

在解码器的最后阶段，包含一个线性层和一个Softmax层，它们的作用时把解码器输出的表示转化为词汇表的概率分布，这个词汇表的概率分步代表下一个被生成token的概率，其本质上就是在猜下一个最可能的输出，至于输出是否符合客观事实，模型无从得知，模型就会一本正经的胡说八道，这种情况也被称之为“幻觉”

<img src="img/AI科普/image-20240122145435673.png" alt="image-20240122145435673" style="zoom:67%;" />

解码器的一整个流程会重复多次，新的token会持续生成，直到生成的是一个用来表示输出序列结束的特殊token，这样就有了来自解码器的完整输出序列

<img src="img/AI科普/image-20240122145936124.png" alt="image-20240122145936124" style="zoom:67%;" />

## 架构变种

<img src="img/AI科普/image-20240122150120757.png" alt="image-20240122150120757" style="zoom:67%;" />

**仅编码器**

<img src="img/AI科普/image-20240122150203401.png" alt="image-20240122150203401" style="zoom:67%;" />

**仅解码器**

<img src="img/AI科普/image-20240122150234376.png" alt="image-20240122150234376" style="zoom:67%;" />

**编码器-解码器**

<img src="img/AI科普/image-20240122150326488.png" alt="image-20240122150326488" style="zoom: 67%;" />